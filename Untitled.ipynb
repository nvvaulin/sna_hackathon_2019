{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import parquet\n",
    "train = parquet.read_table('data/collabTrain',columns=[\"instanceId_userId\"]).to_pandas()[\"instanceId_userId\"].values\n",
    "test = parquet.read_table('data/collabTest',columns=[\"instanceId_userId\"]).to_pandas()[\"instanceId_userId\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161059, 237944)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u,c = np.unique(train,return_counts=True)\n",
    "u = u[c>5]\n",
    "_u,_c = np.unique(test,return_counts=True)\n",
    "i = np.intersect1d(u,_u)\n",
    "len(i),len(_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(958415, 1181443)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = set(i)\n",
    "(_c[np.array([j in i for j in _u])]).sum(),_c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import os\n",
    "import tqdm\n",
    "class PredictSeqDataIter(mx.io.DataIter):\n",
    "    def __init__(self,test_path,train_path,batch_size,data_names,label_names=[],shuffle=False,nan_aug=0.,max_len=None,usampling=False,seq_len=8):\n",
    "        assert batch_size % seq_len == 0,'seq_len should devide batch_size'\n",
    "        self.data,self.params = self.load_data(test_path,train_path,data_names+label_names+['instanceId_userId','liked'])\n",
    "        self.nan_aug = nan_aug\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.provide_data = [(i,(self.batch_size,)+self.data[i].shape[1:]) for i in data_names]\n",
    "        self.provide_label = [(i,(self.batch_size,)) for i in label_names]\n",
    "        self.seq_len = seq_len\n",
    "        self.max_len = max_len\n",
    "        self.usampling = usampling\n",
    "        self.reset()\n",
    "        \n",
    "    def load_data(self,test_path,train_path,names):\n",
    "        params = dict([(i,pickle.load(open(train_path+'/'+i+'.pkl','rb'))) for i in names])\n",
    "        data = {}\n",
    "        for i in names:\n",
    "            v = np.load(train_path+'/'+i+'.npy')\n",
    "            if len(v.shape)==1:\n",
    "                v=v.reshape((-1,1))\n",
    "            if(i=='feedback' or i == 'liked'):\n",
    "                v1 = np.zeros_like(v)[:len(v1)]\n",
    "            else:\n",
    "                v1 = np.load(test_path+'/'+i+'.npy')\n",
    "                if len(v1.shape)==1:\n",
    "                    v1=v1.reshape((-1,1))\n",
    "            data[i] = np.concatenate((v,v1),0)\n",
    "            is_train = np.arange(len(data[i])) < len(v)\n",
    "        data['is_train'] = is_train\n",
    "        return data,params\n",
    "    \n",
    "    def reset(self):\n",
    "        self.inx = np.arange(len(self.data[self.provide_data[0][0]]),dtype=np.uint64)[self.data['is_train'].flatten()]\n",
    "        uid = self.data['instanceId_userId'].flatten()\n",
    "        self.inx = self.inx[np.argsort(uid[self.inx])]        \n",
    "        uid = uid[self.inx]\n",
    "        inx = []\n",
    "        uuid = np.unique(uid)\n",
    "        for u in uuid:\n",
    "            i = self.inx[np.searchsorted(uid,u,'left'):np.searchsorted(uid,u,'right')]\n",
    "            inx.append(np.random.choice(i,self.seq_len))\n",
    "        self.inx = np.concatenate(inx)\n",
    "        \n",
    "        pinx = np.arange(len(self.data[self.provide_data[0][0]]),dtype=np.uint64)[self.data['is_train'].flatten()==False]\n",
    "        \n",
    "        for \n",
    "        \n",
    "        self.cur = 0\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        label = self.data['liked'].flatten()\n",
    "        inx = np.arange(len(self.data[self.provide_data[0][0]]),dtype=np.uint64)\n",
    "        pinx = inx[self.data['is_train'].flatten()==False]\n",
    "        tinx = inx[self.data['is_train']==True]\n",
    "        uid = self.data['instanceId_userId'].flatten()\n",
    "        tinx = tinx[np.argsort(uid[tinx])]\n",
    "        tuid = uid[tinx]\n",
    "        inx = []\n",
    "        for j in tqdm.tqdm(pinx):\n",
    "            u = uid[j]\n",
    "            i = tinx[np.searchsorted(tuid,u,'left'):np.searchsorted(tuid,u,'right')]\n",
    "            l = label[i]\n",
    "#             p = np.ones(len(l))\n",
    "#             p[l==0] = 1./(l==0).sum()\n",
    "#             p[l==1] = 1./(l==1).sum()\n",
    "            tmp = np.random.choice(i,self.seq_len-1)\n",
    "            inx.append(np.concatenate((tmp,np.array([j])))) \n",
    "            \n",
    "        self.inx = np.array(inx).flatten()\n",
    "        self.cur = 0\n",
    "    \n",
    "    def preprocess(self,data,params):\n",
    "        data=data.copy()\n",
    "        if(params['type'] == 'regression' or params['type'] == 'onehot'):\n",
    "            data = np.clip(data.astype(np.float32),-1,100)\n",
    "        else:\n",
    "            data = data.astype(np.int32)\n",
    "        if(params['type'] == 'id'):\n",
    "            mask = np.random.rand(len(data))< self.nan_aug\n",
    "            data[mask] = params['unknown']\n",
    "        data[np.isfinite(data)==False] = 0\n",
    "        return data\n",
    "    \n",
    "    def get_feature_size(self, key):\n",
    "        params = self.params[key]\n",
    "        if(params['type'] == 'time'):\n",
    "            return (367,33,9),'embedding'\n",
    "        elif(params['type'] == 'regression'):\n",
    "            return 1,'number'\n",
    "        elif(params['type'] == 'onehot'):\n",
    "            return self.data[key].shape[-1],'number'\n",
    "        elif(params['type'] == 'categorical'):\n",
    "            return len(self.params[key]['unames']),'embedding'\n",
    "        elif(params['type'] == 'id'):\n",
    "            return len(self.params[key]['unames']+1),'embedding'\n",
    "        \n",
    "    def gen_minibatch(self):\n",
    "        inx = self.inx[self.cur:self.cur+self.batch_size]\n",
    "        self.cur += self.batch_size\n",
    "        if(len(inx) < 1):\n",
    "            raise StopIteration\n",
    "        batch = {}        \n",
    "        for k,v in self.data.items():\n",
    "            batch[k] = self.preprocess(v[inx],self.params[k])\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        batch = self.gen_minibatch()\n",
    "        data = []\n",
    "        label = []\n",
    "        for i in self.provide_data:\n",
    "            v = mx.nd.zeros(i[1])\n",
    "            l = len(batch[i[0]])\n",
    "            v[:l] = batch[i[0]]\n",
    "            data.append(v)\n",
    "        for i in self.provide_label:\n",
    "            v = mx.nd.zeros(i[1])\n",
    "            l = len(batch[i[0]])\n",
    "            v[:l] = batch[i[0]].flatten()\n",
    "            label.append(v)\n",
    "        return mx.io.DataBatch(data=data,label=label,pad=(self.batch_size-l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx=[mx.gpu(0),mx.gpu(1)]\n",
    "num_epoch=100\n",
    "exp_dir='exps'\n",
    "load_epoch=0\n",
    "name='seq2seq'\n",
    "batch_size=512\n",
    "seq_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "remove_features = ['instanceId_objectId','instanceId_userId','liked']\n",
    "data_names=[i[:-4] for i in os.listdir('./train_preprocessed') if i.endswith('.npy') and not  (i[:-4] in remove_features)]\n",
    "label_names=['liked']\n",
    "print(len(data_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1181443 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 4/1181443 [00:00<10:43:47, 30.59it/s]\u001b[A\n",
      "  0%|          | 8/1181443 [00:00<10:36:38, 30.93it/s]\u001b[A\n",
      "  0%|          | 37/1181443 [00:00<3:29:27, 94.01it/s]\u001b[A\n",
      "  0%|          | 45/1181443 [00:00<4:15:59, 76.92it/s]\u001b[A\n",
      "  0%|          | 52/1181443 [00:00<5:02:38, 65.06it/s]\u001b[A\n",
      "  0%|          | 58/1181443 [00:01<5:43:49, 57.27it/s]\u001b[A\n",
      "  0%|          | 63/1181443 [00:01<6:04:31, 54.01it/s]\u001b[A\n",
      "  0%|          | 68/1181443 [00:01<6:25:59, 51.01it/s]\u001b[A\n",
      "  0%|          | 72/1181443 [00:01<6:40:22, 49.18it/s]\u001b[A\n",
      "  0%|          | 76/1181443 [00:01<6:52:51, 47.69it/s]\u001b[A\n",
      "  0%|          | 80/1181443 [00:01<7:05:03, 46.32it/s]\u001b[A\n",
      "  0%|          | 84/1181443 [00:01<7:09:08, 45.88it/s]\u001b[A\n",
      "  0%|          | 100/1181443 [00:01<6:24:08, 51.25it/s]\u001b[A\n",
      "  0%|          | 107/1181443 [00:02<6:35:19, 49.80it/s]\u001b[A\n",
      "  0%|          | 116/1181443 [00:02<6:23:57, 51.28it/s]\u001b[A\n",
      "  0%|          | 123/1181443 [00:02<6:40:52, 49.11it/s]\u001b[A\n",
      "  0%|          | 129/1181443 [00:02<6:53:17, 47.64it/s]\u001b[A\n",
      "  0%|          | 138/1181443 [00:02<6:45:12, 48.59it/s]\u001b[A\n",
      "  0%|          | 144/1181443 [00:02<6:42:01, 48.97it/s]\u001b[A\n",
      "  0%|          | 150/1181443 [00:03<6:43:23, 48.81it/s]\u001b[A\n",
      "  0%|          | 157/1181443 [00:03<6:42:04, 48.97it/s]\u001b[A\n",
      "  0%|          | 163/1181443 [00:03<6:40:44, 49.13it/s]\u001b[A\n",
      "  0%|          | 169/1181443 [00:03<6:38:31, 49.40it/s]\u001b[A\n",
      "  0%|          | 187/1181443 [00:03<6:11:44, 52.96it/s]\u001b[A\n",
      "  0%|          | 196/1181443 [00:03<6:18:14, 52.05it/s]\u001b[A\n",
      "  0%|          | 215/1181443 [00:03<5:54:15, 55.57it/s]\u001b[A\n",
      "  0%|          | 226/1181443 [00:04<6:10:03, 53.20it/s]\u001b[A\n",
      "  0%|          | 235/1181443 [00:04<6:19:27, 51.88it/s]\u001b[A\n",
      "  0%|          | 242/1181443 [00:04<6:19:43, 51.84it/s]\u001b[A\n",
      "  0%|          | 249/1181443 [00:04<6:29:23, 50.56it/s]\u001b[A\n",
      "  0%|          | 265/1181443 [00:05<6:14:00, 52.64it/s]\u001b[A\n",
      "  0%|          | 274/1181443 [00:05<6:23:09, 51.38it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0153f9593a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_iter = PredictSeqDataIter('./test_preprocessed','./train_preprocessed',batch_size,\n\u001b[0;32m----> 2\u001b[0;31m                                data_names=data_names,label_names=label_names,shuffle=True,usampling=True,seq_len=32)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6bb7a63a901c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, test_path, train_path, batch_size, data_names, label_names, shuffle, nan_aug, max_len, usampling, seq_len)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6bb7a63a901c>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtinx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m#             p = np.ones(len(l))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#             p[l==0] = 1./(l==0).sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_iter = PredictSeqDataIter('./test_preprocessed','./train_preprocessed',batch_size,\n",
    "                               data_names=data_names,label_names=label_names,shuffle=True,usampling=True,seq_len=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "def make_full_data():\n",
    "    for i in os.listdir()\n",
    "    data = np.load(path+'/feedback.npy')\n",
    "    params = pickle.load(open(path+'/feedback.pkl','rb'))\n",
    "    inx = list(params['unames']).index('Liked')\n",
    "    data = data[:,[inx]]\n",
    "    pickle.dump({'type':'categorical','unames':['NonLiked','Liked']},open(path+'/liked.pkl','wb'))\n",
    "    np.save(path+'/liked.npy',data.astype(np.uint8))\n",
    "    \n",
    "make_like_feature('./train_preprocessed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
